#Hierarchical and Mixed Effects Models

#Video 1
#What is a hierarchical model?
  #data can have several types of structure, including being nested within itself, making the data "hierarchical"
  #Analyze data using lme4 package
  #4 Parts
    #1. go over basic parts of a mixed effects model (e.g. using student test scores)
    #2. Learn how apply and interpret results of a linear mixed-effects regression or model
    #3. Learn how to use a generalized linear mixed effects model
    #4. Apply mixed effect models to time-series analysis as part of a repeated measures analysis
#Why use a hierarchical model?
  #Sometimes data is nested within itself, observations become dependent 
  #Pool information across small sample sizes (unequal sample sizes between groups)
    #e.g. student test scores, nested within classrooms of varying size
    #treating classrooms as a "random effect" within the model, we can pool
    #shared information about means across the classrooms within the same school
  #Repeated observations across groups or individuals
    #repeated-measures analysis counts as a hierarchical model, allows us to correct for this
#Other names for hierarchical models
  #a) Nested models: because data is nested within groups (e.g. students nested in classrooms)
  #b) hierarchical models: because data has a hierarchy
  #c) multi-level models: because we have two or more levels of interest
  #d) Regression framework: "Pool" information, "Random-effect" versus a "fixed effect", 
  #"Mixed-effect" (linear mixed-effect model; LMM), Linear mixed-effect regression (lmer)
  #e)a multilevel covariate may sometime be call a "random effect" that "pools" information across groups
  #f) Repeated sampling: "repeated measures", "Paired tests"
#Example school test scores
  #examine sample of kindergarten and first grade students; assess how their math knowledge improved
  #Several datasets at different levels: 
    #student level variables: student id, math test-score gain, math kindergarten score, gender, minority status
    #classroom-teacher level: 
    #school level
#Exploring multiple levels: classrooms and schools
  #Students learn within classrooms and classrooms exist withinn schools
  #aggregating student scores by classroom/school can account for lack of independence
  #hierarchical models avoid aggregation by modeling this structure
  
#Video 2
#Parts of a regression
  #Slopes and intercepts
  #linear regression == linear model
  #in basic model, we examine mean across all groups
    #given the term beta: it's the intercept
    #also include error term, epsilon
    #y = B + e
#Intercepts
  #For models with multiple intercepts (e.g. two groups, discrete predictors), we can structure the model two ways
    #1. use a global intercept beta0 and then model the effects of groups 2 and 3 compared to the global intercept
    y = B0 + B2x2 + B3x3 + e
    #2. model each group as its own intercept
    y = B1x1 + B2x2 + B3x3 + e
  #Use option 1 if: interested in estimating how two treatments differed from a reference
  #Use option 2 if: interested in estimating the mean of each group
    #note option 2 is essentially an Analysis of Variance (ANOVA)
#Linear models in R
  #the base function lm() builds linear models
  lm(formula, data)
  lm( y ~ x, data = myData)
  #to run an ANOVA, we can wrap the linear model's output in the anova() function
  anova(lm(y~x, data = myData))
  #hence you can use intercepts to build a linear model for discrete predictor variables
  #Slopes are for looking at continuous predictor variables
#A simple linear regression with slopes
  #the most basic regression includes a response variable, y, an intercept, beta0, a slope beta1, and an erro term
  y ~ B0 + B1x + e
  #The slope predicts how the expected value changes because of the continuous predictor
#Multiple regression
  #The simple linear regression can expand to include multiple response variables
  y ~ B0 + B1x1 + B2x2 + ... + e
  #These different response can include both discrete predictors (with corresponding intercepts)
  #and continuous predictors with corresponding slopes
#Multiple regression caveats
  #things to note about multiple regression
  #1. Independence of predictor variables: predictor variables can change the estimates
  #for other predictors if both are not independent (small deviations can cause quantitative changes to the model if parameters are added or dropped)
  #large deviations can cause qualitative and statistical changes to the model
  #2. The previous consideration requires us to say "estimates have been corrected fro" other variables
  #3. Simpson's paradox: missing important predictors can make our model wrong
  #4. Only linear: multiple regression assumes linearity
  #5. We may need to consider interactions, which occur when groups have different slopes
  #linear models in R is relatively straightforward but has some quirks
#Multiple regression in R tips
  #1.  a minus 1 is needed to estimate an intercept for each group rather than the intercept relative to the first group
  lm(y ~ x - 1) - estimates an intercept for each x
  #2. if a predictor x is numeric, R treats it as a slope
    #fix this by converting predictors to factors
  #3. predictors may need scaling; if one predictor is in cm and the other is in 1000 cm
    #using km for second predictor would be more appropriate
    #when using time for a slope estimate, you may want to use rescale
    #use 0 for the start
  #4.interaction shortcuts; use * for interactions
  lm(y ~ x1 + x2 + x1:x2) == lm(y ~ x1 * x2)
#Refresher of running and plotting a linear regression in R
regModel <- lm(response ~ predictor, data = regDemo
summary(regModel)
regModel
regCoefPlot <- tidy(regModel) # extract coefficients using tidy function from broom:: 

ggplot(regDemo, aes(x = predictor, y = response) ) +
geom_point() + 
theme_minimal() + 
geom_abline( intercept = regCoefPlot$estimate[1] ##better than geom_smooth to use abline, as abline extends to complicated models
             slope = regCoefPlot$estimate[2] )
  
#Video 3
#Random-effects in regressions
  #when variables are nested within another distribution
  #e.g. students nested in classrooms, nested in schools
  #nested relationships are called "multi-level models" 
  #mathematically, this is called a mapping from one distribution to another
  #practically, MLM gives us a way to pool/share information across replicates
  #thus, MLM manages outliers; outliers from groups with small sample sizes have less
    #of an impact if we treat it as a random effect
  #simple random effect has the following equation (algebraic notation):
    #y ~ Bix + e --- aka. relationship to data give i-th beta
    #Bi ~ Normal(mu,sigma) --- random effect assumes Beta is drawn from a normal distribution with mean, mu, and DS, sigma
  #In R, we use lme4 package; note: lme4 is upgrade of lmer package
library(lme4)
lmer( y ~ x + 1|randomGroup), data = myData)
lmer( y ~ x + randomSlope|randomGroup), data = myData)
  #with lme4 notation, need to specify a random effect
  #to specify a random intercept, we use parentheses with a one-pipe-random effect group
lmer( y ~ x + 1|randomGroup), data = myData
  #to specify a random slope, we use parentheses slope - pipe - random effect group
lmer( y ~ x + (randomSlope|randomGroup), data = myData)
  #Random-effect intercepts
    #fixed-effect parameters only use data for a specific group.
    #random-effect parameters assume data share a common distribution
      #for situations with small amounts of data or outliers, random-effect models can produce different estimates
    #lme4 package fits mixed-effect models (models with both fixed- and random-effects) with lmer()
      #random-effect intercepts use special syntax
 lmer( y ~ x + (1|randomEffect), data = myData)

  #Random-effect slopes
    #when lme4 estimates a random-effect slope, it also estimates a random-effect intercept
    #lmer() uses ( continuousPredictor | randomEffectGroup) for a random-effect slope
  #you can extract and plot the fitted model using ggplot2  
    #ggplot2 can plot many models using geom_smooth() or stat_smooth() but not all models
    #one trick to plot models not part of ggplot2 is to use the predict() function
    #predict can help you extract the predicted values for a fitted model.
    #These points can be plotted with ggplot2 using geom_line()
  
#Video 4
#School data
  #Applying MLM to school data
  #school dataset: examine if different factors affect how students learn math
  #1. does sex of a student impact knowledge gain?
  #2. does teacher's training impact gain and does the teacher's math knowledge impact gain
  #3. plot the parameter estimates using ggplot2
#Plotting and interpreting results
  #you can usually see regression coefficients using summary()
  #plotting the regression coefficients and their 95% CI can be another way to visualize and interpret
    #a positive coefficient means, coefficient increase, expected response increases too
    #if the 95% CI doesn't include zero, the coefficient can be considered statistically significant

#Video 5
#Linear mixed effect model - Birth rates data
  #Examine birth rates across counties
  #Using a random-effects intercept model
  #plot dataset, run lmer, examine importance of regression coefficients, learn how to summarize results
  #second task: apply random-effect slopes model to crime data from Maryland
#Birth Rates Data
  #Small populations can be subject to random stochastic (randomly determined) - aka. demographic stochasticity
  #random-effects is one solution to this problem
  #e.g. assume avg birth rate = 1 baby / 10 women of childbearing age
    #In pop of 10000 women, expect 1000 births/year
    #In pop of 10 women, can expect anywhere from 0 - 10 births because chance
  #US - each state has separate "counties" (Louisiana = "Parishes")
    #marketing firm may want to know which counties have higher birth rates (sell more baby ads)
    #political scientist may examine how policies vary across counties, affect birth rates
  #Case study: does mother's age impact birth rate
    #plot shows each county's total pop on X-axis, birth rate on y-axis (Funnel shape)
    #see that smaller counties have wide mix of high & low birth rates
      #due to demographic stochasticity; higher pop counties stick closer to average
#LMER syntax in R
  #lmer() in lme4 package uses formula similar to linear model in base R
library(lme4)
lmer( y ~ x + (Random-effect), data = myData )
  #note: lmer() must include random-effect in parentheses or returns an error
  #random-effect can take different forms
    #random intercept with a fixed mean       -- ( 1 | group ):         Random intercept with fixed mean
    #nested intercepts                        -- ( 1 | gl/g2 ):         Intercepts vary among g1 & g2 within g2
    #multiple intercepts                      -- ( 1 | g1) + (1 | g2):  Random intercepts for 2 variables
    #correlated random slopes & intercepts    -- x + (x | g):           Correlated random slope and intercept
    #uncorrelated random slopes & intercepts  -- x + (x || g):          Uncorrelated random slope and intercept
#Exercise 1: Random-effect intercepts
  #Start off with building a global intercept model
globalIntercept <- lmer(y ~ (1|random intercept), data = myData)
summary(globalIntercept)
  #Then you can add additional fixed effect predictors
    #Numeric variables that are added as fixed effects, will have the corresponding coefficient treated as a slope
#Exercise 2: Random-effect slopes
  #last exercise let you account for each state having its own intercept
  #this exercise, you estimate a random-effect slope for each state
  #e.g. maybe log10 (total pop of each county) changes birth-rate and varies by state
  #recall you specify RE slope using (slope|group)
#Exercise 3: Uncorrelated random-effect slope
  #default settings on lme4 assumes slopes and intercept within each group were correlated for the random-effect estimates
  #using uncorrelated random-effects is 1 method to simplify the model
  #to fit uncorrelated random-effect slope, use || rather than | within lmer() syntax
  #Note that sometimes setting this returns a "singular fit" warning
    #singular fit suggests a more complex model is required
    #The Corr column denotes the correlation between the RE slope and the RE intercept
    #if the Corr is perfectly negative, one cannot estimate independent of each other
#Exercise 4: Fixed and random-effect predictor
  #Sometimes models can have the same predictor as both a fixed and random effect
  #e.g. estimating the effect of the average age of mothers in a particular county on birth rates
    #as a fixed effect: you estimate the effect of  amother's age across all locations
    #as a random efect: you account (or correct) for different slope estimates among states

#Video 6
  #Understanding and reporting the output of a lmer
  #How to extract and understand the output from these mixed-effect models
  #Refresher: we built a model to predict the birth rate for each county 
    #this model includes the average age of the mother within each county as a fixed-effect 
    #and as a random-effect within each state; specified State as a random intercept
out <- lmer(BirthRate ~ AverageAgeofMother + (AverageAgeofMother|State), data = countyBirthsData)
  #typing "out" in the console prints the model's output
  #several extra features over and above a linear model
    #1. fit by REML: we fit model using REML (Restricted maximum likelihood method) - used because numerically
      #fitting a mixed-effects model can be difficulty and normal maximum likelihood (ML) often fails
    #2. input formula
    #3. REML criterion at convergence: model diagnostic, useful if model not converging
    #4. Random effects table: shows standard deviations for the random effect and the residuals
    #5. number of observations and groups
    #6. Fixed effects table: similar to output of a linear model
  #summary() produces additional details
    #1. summary details on residuals: e.g. shows variance now too
    #2. fixed effects now include SE & t-values, but not p-values
    #3. Correlation of Fixed Effects: correlations of the estimators with the intercept
      #generally, if we "repeat" the experiment, we would expect a negative correlation b/w fixed-effect & intercept
