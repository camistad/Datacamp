#Classification with nearest neighbors
  #Machine learning: utilizes computers to turn data into insight and action
    #subset of machine learning = supervised learning: training a machine to learn from prior examples
    #classification: concept to be learned is a set of categories
      #e.g. identifying diseases, is image a cat, etc.
  #Classification tasks for driverless cars
    #e.g. classifying signs, objects in distance
  #Understanding nearest neighbors
    #to start training driverless car, would supervise by demonstrating desired behavior
    #as it observes each sign
    #after some time under instruction, vehicles builds database recording sign & target behavior
    #imagine a set of similar images:
      #a nearest neighbor classifier takes advantage of fact that
      #signs that look alike should be similar to/nearby other signs of same type
    #measuring similarity with distance
      #how nearest neighbor learner decide whether two signs are similar
      #it doesn't measure distance between signs in physical space - but instead it 
      #imagines the properties of signs as coordinates in feature space
      #E.g. consider color - is imagined as 3D feature space, signs of similar color are
      #located naturally close to one another
      #then measure distance through geometric formula (e.g. eucledian geometry)
    #Applying nearest neighbors in R
      #An algorithm called k nearest neighbors (knn) uses the principal of nearest neighbors
      #to classify unlabeled examples
      #R's KNN function searches the dataset for observation most similar to newly observed observation
    #knn function: in "class" library
      library(class)
      pred <- knn(training_data, testing_data, training_labels)
      knn(train = x, test = y, cl = z)
      #requires 3 parameters: 1) set of training data, 2) test data to be classified, 3) labels for training data 
      #train: past data of columns of different predictors 
      #test: new data of columns of different predictors
      #cl: the actual labels from the train data
       #knn function predicts what the labels for the test data would be
       #i.e. what to classify each test data row
       #can use table(test actual, knn predicted) to identify any patterns in correct/incorrect
       #can use mean(test actual == knn predicted) to determine percent correct
  #What about the 'k' in kNN?
    #The letter k is a variable that specifies the number of neighbors to consider when making the classification
    #similar to "number of neighborhoods"
    #ignored k for now, therefore R uses default value of 1
    #therefore only the single nearest, most similar, neighbor was used to classify the unlabeled example
      #Example here: show how k can have substantial impact on performance of classifier
      #Vehicle observes 1 crossing sign with 5 nearest neighbors (2,3,4 crossing, 1, 5 speed limit)
      #nearest neighbor 1 is a speed limit sign, but shares very similar background color
      #knn classifier with k set to one would make an incorrect classification
      #note that 2, 3, 4 are correct
      #now suppose set k to three - then 3 nearest neighbors (1, 2, 3) would take a vote
      #here there would be 2 crossing signs and 1 speed limit sign
      #category with the majority of nearest neighbors (crossing sign)
      #similar if k set to five, then 5 nearest neighbors would vote
      #still choose crossing sign as 3 crossing signs to 2 speed limit signs wins
      #if tie, winner decided at random
    #note: bigger k != better
    #smaller k -> subtle patterns - small neighborhoods
    #larger k -> fuzzy boundary - allows for some other factor that can add randomness to data
    #no universal number to set k to - depends on pattern to be learned and impact of noisy data
    #rule of thumb is to start with k equal to sqrt of number of observations in training data
      #e.g. if car had 100 previously observed road signs - set k to 10
      #make sure to test several different values and see what happens
    #set k in knn() function with k = # within knn()
  #Data preparation for kNN
    #need to prepare data for nearest neighbors
    #kNN assumes numeric data
      #nearest neighbor learners use distance functions to identify most similar or nearest examples
      #many common distance functions assume data are in numeric format
      #difficult to define distance between categories
      #e.g. color defined by numbered color intensity (r/g/b)
      #if have property that can't be defined numerically (e.g. shape: rectangle, diamond, etc>)
      #common solution uses 1/0 indicators (dummy variables) to represent categories
    #dummy coding
      #binary "dummy" variable is create for ech category except one
      #this variable is set to "1" if the category applies and "0" otherwise
      #the category left out can be deduced
      #e.g. assume 3 categories: rectangle, diamond, octagon
      #create 2 dummy variables for rectangle & diamond
      #if rectangle, rectangle set to 1, diamond set to 0 (and vice versa)
      #if octagon: both rectangle & diamond set to 0
      #dummy coded data can be used directly in a distance function
      #two rectangle signs with values of 1 will be found to be closer together than 
      #a rectangle and a diamond
    #kNN benefits from normalized data
      #when calculating distance, each feature of the input data should be measured with 
      #the same range of values
      #e.g. traffic sign data - each color component ranged from a minimum of zero to a max of 255
      #suppose that we added 1/0 dummy variables for sign shapes into distance calculations
      #two diff shapes differ by only 1 unit, but 2 diff colors can differ by 255 units
      #different scales allows feature with wider range to have more influence
      #compressing color to follow 0-1 range corrects for issue
      #R does not have a built-in function to rescale data to a given range
      normalize <- function(x) {
         return((x - min(x)) / (max(x) - min(x)))
      }
      #this normalize() function can be used to perform min-max normalization
      #rescales any vector x so that its minimum value is zero and max value is one
      #subtracts minimum value from each value of x and divdes by range of x values
#Understanding Bayesian methods
  #Some smartphones / apps offer routes & traffic estimates w/o user even asking
  #phone keeps record of user's past locations
  #uses data to forecast user's most probable future location
  #Bayesian methods proposes rules for estimating probabilities in light of historic data
  #estimating probability
    #e.g. map tracking # of times attended at 4 different locations
    #Home = 10, Work = 23, restaurant = 3, store = 4
    #home predicted at 57.5% or 23/40 times
    #Probability of A is denoted P(A)
    #P(work) = 23/40  = 57.5%
    #P(store) = 4/40 = 10%
    #Probability = # times event happened / # times could have happened
    #to be more accurate - must be able to combine information from several events into a single probability estimate
  #Joint probability and independent events
    #When events occur together - they have a joint probability
    #create venn diagrams to determine joint probability
    #joint probability of two events is computed by finding proportion of observations in which they occurred together
    #Joint probability of events A and B is denote P(A and B)
    #P(work and evening) = 1%
    #P(work and afternoon) = 20%
  #independent events
    #when 2 events don't go together
    #however, many data elements that do occur together
    #when 1 event is predictive of another, they're called dependent
  #conditional probability expresses how 1 event depends on another
    #conditional probability of events A & B is denoted P(A|B)
    #P(A|B) = P(A and B) / P(B)
    #Probability of event A given B is equal to their joint probability divided by probability of B
    #e.g. P(work|evening) asks - what is probability they are at work, given it is the evening?
      #take probability it is evening, divide by probability of them being at work
  #Naive Bayes
    #applies bayesian methods to estimate the conditional probability of an outcome
    #naivebayes packages
    library(naivebayes)
    m <- naive_bayes(location ~ time_of_day, data = location_history)
    P(A|B) ~~ P(Y|X) ~~ P(outcome | predictor)
    #predict() function computes conditional probabilities to predict a future location based on the future conditions
    future_location <- predict(m, future_conditions)
    #typing name of the model object provides the a priori (overall) and conditional probabilities of each of the model's predictors
    #R can compute posterior (predicted) probabilities if you specify type = "prob" in predict() function
  #Understanding NB's "naivety"
    #can build more sophisticated models to get additional data points
    #adding more predictors complicates things
    #with a single predictor, conditional probability is based on overlap b/w two events
    #naive bayes algorithm uses shortcut to approximate conditional probability hoped to compute
    #rather than treating problem as intersection of all related events
    #algorithm makes "naive" assumption of data - assumes events are indep't 
    #by assuming independence - joint probability can be computed by multiplying indeividual probabilities
    #Reduces algorithm's need to observe all possible intersection in the full venn diagram
    #Rather, multiplies probabilities from series of simpler intersections
    #although naive assumption is not true in practice - still performs admirably on many real world tasks
    #An "infrequent" problem
      #in situations where no overlap between two events, the joint probability is 0
      #whenever 0 is multiplied in a chain, entire sequence has zero
      #no matter how overwhelming rest of evidence, any predicted probability of work on a weekend = 0
      #usually add 1 to each event to make sure there's some overlap
      #Laplace correction / Laplace estimator
      naive_bayes(outcome ~ predictor, data = df, laplace = 1) 
      #laplace = 1 sets correction
 #Making binary predictions with regression
    #regression methods are the most common forms of machine learning
    #Introducing linear regression  
    #predicting outcome (Y) from number of X's
    #Y e.g. income, & X e.g. age, education
    #Linear regression involves fitting straight line to data that best captures relationships
  #Regression for binary classification
    #here points fall in two flat rows, 1 or 0 in Y
    #so model an S curve - logistic regression
    #S shaped curve (logistic function)
    #has property: for any input value of x, output is always between 0 and 1 - like probability
    #the greater the probability, the more likely the outcome is to be labelled 1
    #R uses glm function
    m <- glm(y ~ x1 + x2 + x3, data = my_dataset, family = "binomial")
    #family parameter specifies type of regression to be conducted
    prob <- predict(m, test_dataset, type = "response")
    #type = "response" produces predicted probabilities which are easier to interpret than default log odds
    #if you don't specify type = "response" then the predict() outputs predictions in terms of log odds
    #to make predictions, probabilities must be converted into outcome of interest using ifelse()step
    pred <- ifelse(prob > 0.50, 1, 0)
    #ifelse predicts 1 if probability > 50% or 0 otherwise
  #Model performance tradeoffs
    #rare events lead to tradeoffs for classification models
    #when one outcome is very rare, predicting the opposite can result in a very high accuracy
    #Can be necessary to sacrifice overall accuracy to better target outcome of interest
    #e.g. if you have a more accurate model that's only good at predicting the bad outcome, 
    #may want to find a less accurate model if it is better at predicting when there's a good outcome
  #Understanding ROC cruve
    #provides way to better understand models ability to distinguish between positive and negative predictions
    #e.g. outcome of interest vs. all others
    #imagine you're working on project with positive outcome x & negative outcome 0
    #if classifier is poor, X's and O's are very mixed
    #ROC curve depicts relationship between percentage of positive examples as it relates
    #to percentage of other outcomes
    #When X's & O's are even, then diagonal line, as proportion of interesting examples
    #rises evenly with proportion of negative examples
    #BUT: if you have machine learning model that can sort model to push outcomes to left of dataset
    #Then ROC curve is drawn, it's no longer on diagonal, as model can identify several positive examples
    #for each negative example it accidentally prioritized
    #diagonal line acts as baseline performance for a very poor model
    #the further another curve is from the diagonal line, the better it performs
    #To quantify performance, AUC is used (Area Under Curve) 
    #AUC measures area under ROC curve
    #baseline has AUC = .5
    #Perfect model has AUC = 1, curve all the way in upperleft of square
    #AUC closer to 1 = better - but can be misleading
    #Curves of varying shape can have equal AUC, 
    #But need to see how shape of each curve is and therefore, model's ability to identify
    #e.g. some models may be able to identify easy cases, but not hard ones 
    #ROC curves are important tool for comparing models and determining best model for project
    #when used with a single model, it can help visualize tradeoff 
    #between true positives and false positives for outcome of interest
    #pROC package has roc() function for creating ROC curves and auc() function for judging auc
    library(pROC)
    x<-roc(column of actual, column of predicted probabilities)
    auc(x)
  #Dummy variables, missing data, and interactions
    #All predictors used in regression must be numeric
    #must turn categorical data into number - e.g. with dummy variables
    #missing data is also problematic
    #Also sometimes need to model interactions among predictors
  #Dummy coding categorical data
    #creates a set of binary (one-zero) variables that represent each category
    #except 1 that serves as reference group
    #glm() function can automatically dummy code any factor type variables used in model
    my_data$gender <- factor(my_data$gender,
                      levels = c(0, 1, 2),
                      labels = c("Male", "Female", "Other"))
    #Simply apply factor() function to the data
    #note how a categorical feature may be represented as numbers (1, 2, 3)
    #may still want to factor it - make sure it's treated appropriately
  #Imputing missing data
    #by default, model will exclude any observation w/ missing values on its predictors
    #with categorical missing data, can just treat as missing
    #can fix with imputation - impute data with some guess on what data can be
    #e.g. mean imputation
    #since there may be systematic reasons for missing data
    #can add a binary 1/0 missing value indicator to model to note fact that value was imputed
  #interaction effects
    #2 predictors, when combined, can have different impact on outcome 
    #combination may increase, decrease, or eliminate impact of individual predictosr
    #being able to model combinations is important for creating best predictive models
    #R formula interface uses * to create interaction
    #resulting model will include terms of individual and combined (interaction) effects
  #Automatic feature selection
    #Unlike other ML methods, regression asks human to specify the model's predictors ahead of time
    #Therefore, each of the donation models required some subject matter expertise to identify best predictor
    #Automatic feature selection can be used - especially if have too many predictors
  #Stepwise regression
    #Building rergession model step by step, evaluating each predictor to see which ones add value to the final model
    #backward stepwise: model contains all predictors first, then removes one step by step
      #if removing does not substantially impact ability to predict, then stays removed
      #at each step, remove the assumed to be least impacting predictor
    #foreward stepwise: model contains no predictors, examines each potential predictor 
      #add predictors step-by-step until no predictors add substantial value to model
    #note that backward/foreward selection don't result in same model
    #statisticians also raise concerns that stepwise models may impact assumptions
    #model may over or understate importance of some of the predictors
    #stepwise regression allows model to be built in absence of theory
  #Adding all predictors to dataframe
    glm(outcome ~ ., data = df, family = "binomial")
    #note how you specify all with .
    #if you want to remove any specific predictors  - e.g. predictorone
    glm(outcome ~ . - predictorone, data = df, family = "binomial")
    #if you want to specify an interaction, must do so manually
    glm(outcome ~ . + predictorone * predictortwo, data = df, family = "binomial")
