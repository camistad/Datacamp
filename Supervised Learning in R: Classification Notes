#Classification with nearest neighbors
  #Machine learning: utilizes computers to turn data into insight and action
    #subset of machine learning = supervised learning: training a machine to learn from prior examples
    #classification: concept to be learned is a set of categories
      #e.g. identifying diseases, is image a cat, etc.
  #Classification tasks for driverless cars
    #e.g. classifying signs, objects in distance
  #Understanding nearest neighbors
    #to start training driverless car, would supervise by demonstrating desired behavior
    #as it observes each sign
    #after some time under instruction, vehicles builds database recording sign & target behavior
    #imagine a set of similar images:
      #a nearest neighbor classifier takes advantage of fact that
      #signs that look alike should be similar to/nearby other signs of same type
    #measuring similarity with distance
      #how nearest neighbor learner decide whether two signs are similar
      #it doesn't measure distance between signs in physical space - but instead it 
      #imagines the properties of signs as coordinates in feature space
      #E.g. consider color - is imagined as 3D feature space, signs of similar color are
      #located naturally close to one another
      #then measure distance through geometric formula (e.g. eucledian geometry)
    #Applying nearest neighbors in R
      #An algorithm called k nearest neighbors (knn) uses the principal of nearest neighbors
      #to classify unlabeled examples
      #R's KNN function searches the dataset for observation most similar to newly observed observation
    #knn function: in "class" library
      library(class)
      pred <- knn(training_data, testing_data, training_labels)
      knn(train = x, test = y, cl = z)
      #requires 3 parameters: 1) set of training data, 2) test data to be classified, 3) labels for training data 
      #train: past data of columns of different predictors 
      #test: new data of columns of different predictors
      #cl: the actual labels from the train data
       #knn function predicts what the labels for the test data would be
       #i.e. what to classify each test data row
       #can use table(test actual, knn predicted) to identify any patterns in correct/incorrect
       #can use mean(test actual == knn predicted) to determine percent correct
  #What about the 'k' in kNN?
    #The letter k is a variable that specifies the number of neighbors to consider when making the classification
    #similar to "number of neighborhoods"
    #ignored k for now, therefore R uses default value of 1
    #therefore only the single nearest, most similar, neighbor was used to classify the unlabeled example
      #Example here: show how k can have substantial impact on performance of classifier
      #Vehicle observes 1 crossing sign with 5 nearest neighbors (2,3,4 crossing, 1, 5 speed limit)
      #nearest neighbor 1 is a speed limit sign, but shares very similar background color
      #knn classifier with k set to one would make an incorrect classification
      #note that 2, 3, 4 are correct
      #now suppose set k to three - then 3 nearest neighbors (1, 2, 3) would take a vote
      #here there would be 2 crossing signs and 1 speed limit sign
      #category with the majority of nearest neighbors (crossing sign)
      #similar if k set to five, then 5 nearest neighbors would vote
      #still choose crossing sign as 3 crossing signs to 2 speed limit signs wins
      #if tie, winner decided at random
    #note: bigger k != better
    #smaller k -> subtle patterns - small neighborhoods
    #larger k -> fuzzy boundary - allows for some other factor that can add randomness to data
    #no universal number to set k to - depends on pattern to be learned and impact of noisy data
    #rule of thumb is to start with k equal to sqrt of number of observations in training data
      #e.g. if car had 100 previously observed road signs - set k to 10
      #make sure to test several different values and see what happens
    #set k in knn() function with k = # within knn()
  #Data preparation for kNN
    #need to prepare data for nearest neighbors
    #kNN assumes numeric data
      #nearest neighbor learners use distance functions to identify most similar or nearest examples
      #many common distance functions assume data are in numeric format
      #difficult to define distance between categories
      #e.g. color defined by numbered color intensity (r/g/b)
      #if have property that can't be defined numerically (e.g. shape: rectangle, diamond, etc>)
      #common solution uses 1/0 indicators (dummy variables) to represent categories
    #dummy coding
      #binary "dummy" variable is create for ech category except one
      #this variable is set to "1" if the category applies and "0" otherwise
      #the category left out can be deduced
      #e.g. assume 3 categories: rectangle, diamond, octagon
      #create 2 dummy variables for rectangle & diamond
      #if rectangle, rectangle set to 1, diamond set to 0 (and vice versa)
      #if octagon: both rectangle & diamond set to 0
      #dummy coded data can be used directly in a distance function
      #two rectangle signs with values of 1 will be found to be closer together than 
      #a rectangle and a diamond
    #kNN benefits from normalized data
      #when calculating distance, each feature of the input data should be measured with 
      #the same range of values
      #e.g. traffic sign data - each color component ranged from a minimum of zero to a max of 255
      #suppose that we added 1/0 dummy variables for sign shapes into distance calculations
      #two diff shapes differ by only 1 unit, but 2 diff colors can differ by 255 units
      #different scales allows feature with wider range to have more influence
      #compressing color to follow 0-1 range corrects for issue
      #R does not have a built-in function to rescale data to a given range
      normalize <- function(x) {
         return((x - min(x)) / (max(x) - min(x)))
      }
      #this normalize() function can be used to perform min-max normalization
      #rescales any vector x so that its minimum value is zero and max value is one
      #subtracts minimum value from each value of x and divdes by range of x values
      
      
